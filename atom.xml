<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[syslog.warten.de]]></title>
  <link href="http://sw0x2A.github.com/atom.xml" rel="self"/>
  <link href="http://sw0x2A.github.com/"/>
  <updated>2013-05-26T10:47:52+02:00</updated>
  <id>http://sw0x2A.github.com/</id>
  <author>
    <name><![CDATA[SW]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Build Spotify RPM on Fedora]]></title>
    <link href="http://sw0x2A.github.com/blog/2013/05/26/build-spotify-rpm-on-fedora/"/>
    <updated>2013-05-26T10:32:00+02:00</updated>
    <id>http://sw0x2A.github.com/blog/2013/05/26/build-spotify-rpm-on-fedora</id>
    <content type="html"><![CDATA[<p><a href="http://leamas.fedorapeople.org/spotify/">Fedora people</a> provide everything to build a RPM package of the latest Spotify client.</p>

<pre><code>sudo yum install rpm-build rpmdevtools yum-utils
rpmdev-setuptree
cd $HOME/rpmbuild/SPECS
wget http://leamas.fedorapeople.org/spotify/0.9.0/spotify-client.spec
cd $HOME/rpmbuild/SOURCES
spectool -g ../SPECS/spotify-client.spec
cd -
sudo yum-builddep spotify-client.spec
QA_RPATHS=$[2|8] rpmbuild -bb spotify-client.spec
rpm -Uvh $HOME/rpmbuild/RPMS/x86_64/spotify-client-0.9.0.133.gd18ed58.259-2.fc18.x86_64.rpm
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Backup Synology NAS using rsync]]></title>
    <link href="http://sw0x2A.github.com/blog/2013/05/26/backup-synology-nas-using-rsync/"/>
    <updated>2013-05-26T10:29:00+02:00</updated>
    <id>http://sw0x2A.github.com/blog/2013/05/26/backup-synology-nas-using-rsync</id>
    <content type="html"><![CDATA[<p>Synology NAS devices offer a <a href="http://forum.synology.com/wiki/index.php/Backup_the_Synology_server_to_a_remote_Synology_server_or_RSync_Server">built-in service</a> to backup all configuration, applications and shared folders to a remote rsync server. Below is a short howto describing the steps required to setup the remote rsync services on a CentOS 6 server.</p>

<p>Install rsync and xinetd.</p>

<pre><code>yum install rsync xinetd
</code></pre>

<p>Create configuration file.</p>

<pre><code># /etc/rsyncd.conf
gid = nobody
uid = nobody
read only = no
use chroot = yes 
transfer logging = true
log format = %h %o %f %l %b
log file = /var/log/rsyncd.log
secrets file = /etc/rsyncd.secrets
hosts allow = 192.168.1.0/24

[synology]
comment = Backup of Synology
path = /data/synology
auth users = synology
</code></pre>

<p>Create password file.</p>

<pre><code># /etc/rsyncd.secrets 
synology:plaintextpassword
</code></pre>

<p>Restrict its file permissions.</p>

<pre><code>chmod 600 /etc/rsyncd.secrets
</code></pre>

<p>Enable rsync service in xinetd by setting <code>disabled = no</code> in <code>/etc/xinetd.d/rsync</code> and restart xinetd.</p>

<p>Open rsync port in iptables and make the change permanent.</p>

<pre><code>iptables -I INPUT -p tcp -m state --state NEW -m tcp --dport 873 -j ACCEPT
service iptables save
</code></pre>

<p>Finally, fix selinux config and context of the destination directory.</p>

<pre><code>setsebool -P allow_rsync_anon_write=1
chcon -t public_content_rw_t /data/synology
</code></pre>

<p>The Synology NAS should be able to establish a connection to the remote rsync server and do backups.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hello world]]></title>
    <link href="http://sw0x2A.github.com/blog/2013/03/16/hello-world/"/>
    <updated>2013-03-16T18:56:00+01:00</updated>
    <id>http://sw0x2A.github.com/blog/2013/03/16/hello-world</id>
    <content type="html"><![CDATA[<p>It is the fourth time that I post a Hello world in this blog. I did this whenever I changed the underlying blogging software. It started with Movable Type, a bit later I added a microblog in the same software. After years I merged both into Wordpress and with this post I am going to relaunch it using Octopress.</p>

<pre><code>package main

import "fmt"

func main() {
    fmt.Println("Hello, world")
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Simple SFTP setup]]></title>
    <link href="http://sw0x2A.github.com/2012/06/simple-sftp-setup/"/>
    <updated>2012-06-18T00:00:00+02:00</updated>
    <id>http://sw0x2A.github.com/2012/06/simple-sftp-setup</id>
    <content type="html"><![CDATA[<h1></h1>

<p>SFTP is a subsystem of SSH and quiet easy to set up. It has some limitations compared with FTP and it is not to be confused with FTPS (FTP over SSL/TLS). The requirement is just to provide a way to transfer files from and to a server with secure authentication and connection. We also want to the user access be limited to the homedir using SSH ChrootDirectory directive and enforce SFTP protocol (user should not be able to use SSH instead).</p>

<p>All changes that need to be applied on a SSH enabled host are that you have to add /usr/lib/openssh/sftp-server to /etc/shells and replace the Subsystem sftp line in /etc/ssh/sshd_config as below:</p>

<pre><code>#Subsystem sftp /usr/lib/openssh/sftp-server
Subsystem sftp internal-sftp
</code></pre>

<p>Since we want to chroot the user to her homedir and do not want her to see names of other users, we have to create a little mess of subdirectories and chroot here. SSH requires chroot to be one level above homedir.</p>

<p>We need two users per share. One should have read-write permissions (push), the other should have read-only access (pull). The addsftpuser script below creates the users and outputs a part of SSH config that defines some rules for the new users. Those lines should be added to /etc/ssh/sshd_config and SSH should be restarted to load the new config.</p>

<pre><code>#!/bin/bash

set -e

if [ $# -ne 2 ]
then
  echo "Usage: $(basename $0) username environment"
  exit 1
fi

USER=$1_push_$2
USERRO=$1_pull_$2
read -p "Password push-user: " PASS
read -p "Password pull-user: " PASSRO

groupadd ${USER}
GROUP=$(getent group $USER | cut -d: -f3)

mkdir -p /home/sftp/$USER
useradd -d /home/sftp/$USER/$USER -m -s /usr/lib/openssh/sftp-server
-g $GROUP -p $(openssl passwd -1 $PASS) ${USER}
useradd -d /home/sftp/$USER/$USER    -s /usr/lib/openssh/sftp-server
-g $GROUP -p $(openssl passwd -1 $PASSRO) ${USERRO}

cat &lt;&lt;END

To make the user able to connect via SFTP, you MUST add these lines to
your /etc/ssh/sshd_config and restart sshd afterwards:

Match User $USER
    ChrootDirectory /home/sftp/$USER
    AllowTCPForwarding no
    X11Forwarding no
    ForceCommand internal-sftp

Match User ${USERRO}
    ChrootDirectory /home/sftp/$USER
    AllowTCPForwarding no
    X11Forwarding no
    ForceCommand internal-sftp

END
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[425]]></title>
    <link href="http://sw0x2A.github.com/2012/05/425/"/>
    <updated>2012-05-06T00:00:00+02:00</updated>
    <id>http://sw0x2A.github.com/2012/05/425</id>
    <content type="html"><![CDATA[<h1></h1>

<p><a href="http://unix.stackexchange.com/questions/4105/how-do-i-count-all-the-files-recursively-through-directories">How do I count all the files recursively through directories</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Separate webapps from Tomcat]]></title>
    <link href="http://sw0x2A.github.com/2012/03/separate-webapps-from-tomcat/"/>
    <updated>2012-03-31T00:00:00+02:00</updated>
    <id>http://sw0x2A.github.com/2012/03/separate-webapps-from-tomcat</id>
    <content type="html"><![CDATA[<h1></h1>

<p>Until last week I thought it would be necessary to configure Tomcat and its webapps as a big monolithic system. What a great mistake! I have learned that there is a much easier way to run webapps in Tomcat. Actually each webapps becomes its own application server, executed with a dedicated user. I will demonstrate this way by showing how to install Jenkins.</p>

<p>First install <a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">Java JDK</a> and <a href="http://tomcat.apache.org/">Tomcat</a> into /opt. In this case, I have just unpacked the tarballs there and created symlinks to have an easy way to upgrade to newer versions.</p>

<pre><code>root@debian:/opt# ls -l
total 8
drwxr-xr-x 9 root root [...] apache-tomcat-6.0.35
lrwxrwxrwx 1 root root [...] java7 -&gt; jdk1.7.0_03
drwxr-xr-x 8 root root [...] jdk1.7.0_03
lrwxrwxrwx 1 root root [...] tomcat6 -&gt; apache-tomcat-6.0.35
</code></pre>

<p>Add a new user for Jenkins and create the base skeleton of Tomcat directories in its homedir /home/jenkins. Copy the conf directory from your Tomcat installation. The other directories are empty. Download the <a href="http://mirrors.jenkins-ci.org/war/latest/jenkins.war">Jenkins WAR file</a> and save it in the webapps directory.</p>

<pre><code>root@debian:/home/jenkins# ls -l
total 28
drwxr-xr-x 2 jenkins nogroup [...] conf
drwxr-xr-x 6 jenkins nogroup [...] data
drwxr-xr-x 2 jenkins nogroup [...] logs
drwxr-xr-x 3 jenkins nogroup [...] temp
drwxr-xr-x 3 jenkins nogroup [...] webapps
drwxr-xr-x 3 jenkins nogroup [...] work
</code></pre>

<p>Create /home/jenkins/.profile to define the application server environment variables.</p>

<pre><code># Applicationserver enviroment
export JENKINS_HOME=/home/jenkins/data
export JENKINS_BASEDIR=${HOME}
export CATALINA_HOME=/opt/tomcat6
export CATALINA_BASE=${HOME}
export JAVA_HOME=/opt/java7
export CATALINA_PID=${JENKINS_BASEDIR}/jenkins.pid
export CATALINA_OPTS="-Xms2048m -Xmx2048m -Djava.awt.headless=true"
export PATH=${CATALINA_HOME}/bin:${JAVA_HOME}/bin:${PATH}
</code></pre>

<p>Last step is to put an init script for Jenkins into /etc/init.d/jenkins.</p>

<pre><code>#!/bin/bash
#
# Startup / shutdown script for the jenkins server.
#
### BEGIN INIT INFO
# Provides:             jenkins
# Required-Start:       $network $local_fs
# Required-Stop:
# Should-Start:         $named
# Should-Stop:
# Default-Start:        2 3 4 5
# Default-Stop:         0 1 6
# Short-Description:    Jenkins
# Description:          Jenkins Continuous Integration server
### END INIT INFO

SCRIPT_USER=jenkins
LOCKFILE=/var/lock/jenkins
export TOMCAT_HOME=/opt/tomcat6
[ -f $TOMCAT_HOME/bin/catalina.sh ] || exit 0
case "$1" in
  start)
        # Start daemon.
        echo -n "Starting Jenkins: "
        su -l $SCRIPT_USER -c "$TOMCAT_HOME/bin/catalina.sh start"
        RETVAL=$?
        echo
        [ $RETVAL = 0 ] &amp;&amp; touch $LOCKFILE
        ;;
  stop)
        # Stop daemons.
        echo -n "Shutting down Jenkins: "
        su -l $SCRIPT_USER -c "$TOMCAT_HOME/bin/catalina.sh stop"
        RETVAL=$?
        echo
        [ $RETVAL = 0 ] &amp;&amp; rm -f $LOCKFILE
        ;;
  restart)
        $0 stop
        $0 start
        ;;
  condrestart)
       [ -e $LOCKFILE ] &amp;&amp; $0 restart
       ;;
  status)
        status jenkins
        ;;
  *)
        echo "Usage: $0 {start|stop|restart|status}"
        exit 1
esac
exit 0
</code></pre>

<p>Now you can start Jenkins as a seperate service. Repeat the steps from above for each webapp that you need.</p>

<p>This setup makes it possible to start or stop each webapp independent from the others. It also allows you to run webapps with different Java or Tomcat versions or easily change the underlying Java and Tomcat versions (just refer to them in .profile).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Simple and secure Subversion server]]></title>
    <link href="http://sw0x2A.github.com/2012/03/simple-and-secure-subversion-server/"/>
    <updated>2012-03-10T00:00:00+01:00</updated>
    <id>http://sw0x2A.github.com/2012/03/simple-and-secure-subversion-server</id>
    <content type="html"><![CDATA[<h1></h1>

<p>In most environments, people want to access their Subversion repositories over a HTTPS connection. This requires a webserver like Apache, which provides WebDAV and SVN modules. I just switched over to nginx and do not want to install Apache just for SVN only. So I started to search for an alternative method which should be easy and secure.</p>

<p>I found <code>svn ssh</code>. I knew SVN over SSH already but I remembered that there were some problems with file permissions and I did not really liked the idea of having Unix accounts for each SVN user on the system. What I realized then was that there is a pretty nice workaround perfect solution.</p>

<p>First of all, let us create a new SVN user. Only this user will access the repositories.</p>

<pre><code>adduser --home /var/svn --shell /bin/bash --disabled-password svn
</code></pre>

<p>Next step is to create a new repository and import the basic directory structure.</p>

<pre><code>for dir in trunk branches tags; do mkdir -p /tmp/project1/$dir; done
mkdir /var/svn/repos
svnadmin create /var/svn/repos/project1
svn import /tmp/project1 file:///var/svn/repos/project1 -m "Initial import"
</code></pre>

<p>All the magic is done in the SVN user’s SSH <code>authorized_keys</code> file.</p>

<pre><code>command="/usr/bin/svnserve -t -r /var/svn/repos --tunnel-user=user1",no-port-forwarding,no-pty,no-agent-forwarding,no-X11-forwarding ssh-rsa AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA== user1@example.com
command="/usr/bin/svnserve -t -r /var/svn/repos --tunnel-user=user2",no-port-forwarding,no-pty,no-agent-forwarding,no-X11-forwarding ssh-rsa AAAAABBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB== user2@example.com
</code></pre>

<p>You can limit user access per SSH public key. In this case we force the user to execute <code>svnserve</code>, we can limit to repository and we can set a username with <code>--tunnel-user</code> which is used for commits. Additionally, we can disallow all non-SVN stuff to prevent login to shell or port-forwarding etc.</p>

<p>That’s it. Check out your repository and start working with it.</p>

<pre><code>svn co svn ssh://svn@example.com/project1/trunk project1
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DELL XPS 8300 NIC hardware problem]]></title>
    <link href="http://sw0x2A.github.com/2012/03/dell-xps-8300-nic-hardware-problem/"/>
    <updated>2012-03-10T00:00:00+01:00</updated>
    <id>http://sw0x2A.github.com/2012/03/dell-xps-8300-nic-hardware-problem</id>
    <content type="html"><![CDATA[<h1></h1>

<p>It seems like there is a hardware problem at least on some motherboards of <a href="http://en.community.dell.com/support-forums/desktop/f/3514/p/19389902/19906490.aspx">DELL XPS 8300</a>. It appears to be an initialization issue of the network chipset (Broadcom NetLink BCM57788 Gigabit) when you turn on the computer at low temperatures. When OS starts, the NIC is not detected and therefore not usable. As soon as the PC warms up (usually after some minutes), you can restart and the problem is gone until next cold restart.</p>

<p>Seriously, those PCs needs preheating like an old diesel engine. Since there is no choke, you can try with a hair dryer instead. Let’s see how DELL support handles this…</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fetch IP addresses of Pingdom&#8217;s probe servers]]></title>
    <link href="http://sw0x2A.github.com/2011/12/fetch-ip-addresses-of-pingdoms-probe-servers/"/>
    <updated>2011-12-11T00:00:00+01:00</updated>
    <id>http://sw0x2A.github.com/2011/12/fetch-ip-addresses-of-pingdoms-probe-servers</id>
    <content type="html"><![CDATA[<h1></h1>

<pre><code>wget --quiet -O- https://www.pingdom.com/rss/probe_servers.xml | perl -nle 'print $1 if /IP: (([01]?dd?|2[0-4]d|25[0-5]).([01]?dd?|2[0-4]d|25[0-5]).([01]?dd?|2[0-4]d|25[0-5]).([01]?dd?|2[0-4]d|25[0-5]));/'
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install MGSE on other distributions than Linux Mint]]></title>
    <link href="http://sw0x2A.github.com/2011/12/install-mgse-on-other-distributions-than-linux-mint/"/>
    <updated>2011-12-02T00:00:00+01:00</updated>
    <id>http://sw0x2A.github.com/2011/12/install-mgse-on-other-distributions-than-linux-mint</id>
    <content type="html"><![CDATA[<h1></h1>

<p>The friendly guys from <a href="http://linuxmint.com/">Linux Mint</a> wrote some Gnome Shell extentions which make Gnome 3 feel more like Gnome 2. The good thing is that you can benefit from that even if you are using a different distribution. First of all, we will install the <a href="https://github.com/linuxmint/MGSE">Linux Mint Shell Extensions for Gnome 3 (MGSE)</a>.</p>

<pre><code>$ git clone https://github.com/linuxmint/MGSE.git
$ cd MGSE/
$ ./test
</code></pre>

<p>To make the Gnome Shell load the new extentions, reload Gnome Shell by hitting <code>ALT-F2</code> and executing <code>r</code>.</p>

<p>Use <code>gnome-tweak-tool</code> to activate the extentions you want.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Solutions for Tour of Go exercises]]></title>
    <link href="http://sw0x2A.github.com/2011/11/solutions-for-tour-of-go-exercises/"/>
    <updated>2011-11-21T00:00:00+01:00</updated>
    <id>http://sw0x2A.github.com/2011/11/solutions-for-tour-of-go-exercises</id>
    <content type="html"><![CDATA[<h1></h1>

<p>I am learning <a href="http://golang.org/">Go</a> and the <a href="http://tour.golang.org/">Tour of Go</a>, an interactive tutorial, is a good resource to start with. At the end of each section is a series of exercises for the reader to complete.</p>

<p>Unfortunately, there are no example solutions for those exercises, so its hard to get a hint if you are stuck. Therefore I post my solutions here. I do not think they are the best way to solve the problems in Go (as I said, I am learning too) but code is compiling and it is doing what it should. You are welcome to comment with your solutions and code improvements.</p>

<ul>
<li><p><a href="http://tour.golang.org/#44">Exercise: Loops and Functions</a></p>

<pre><code>package main

import (
    "fmt"
    "math"
)

func Sqrt(x float64) float64 {
    var z, d float64 = x, 1
    for d &gt; 1e-15 {
        z0 := z
        z = z-(z*z-x)/(2*z)
        d = z-z0
        if d &lt; 0 {
            d = -d
        }
    }
    return z
}

func main() {
    fmt.Println(Sqrt(2))
    fmt.Println(math.Sqrt(2))
}
</code></pre></li>
<li><p><a href="http://tour.golang.org/#45">Exercise: Maps</a></p>

<pre><code>package main

import (
    "strings"
    "tour/wc"
)

func WordCount(s string) map[string]int {
    m:=make(map[string]int)
    for _, w:=range strings.Fields(s) {
        m[w]++
    }
    return m
}

func main() {
    wc.Test(WordCount)
}
</code></pre></li>
<li><p><a href="http://tour.golang.org/#46">Exercise: Slices</a></p>

<pre><code>package main

import "tour/pic"

func Pic(dx, dy int) [][]uint8 {
    mat := make([][]uint8, dx) 
    for i := range mat { 
        mat[i] = make([]uint8, dy)
    for j := range mat[i] {
        mat[i][j]=uint8(i^j)
    }
    }
    return mat
}

func main() {
    pic.Show(Pic)
}
</code></pre></li>
<li><p><a href="http://tour.golang.org/#47">Exercise: Fibonacci closure</a></p>

<pre><code>package main

import "fmt"

// fibonacci is a function that returns
// a function that returns an int.
func fibonacci() func() int {
    var a, b, count int = 0, 1, 0
    return func() int {
        switch count {
        case 0:  count++
             return a 
        case 1:  count++
             return b
        }
        sum:=a b
        a=b
        b=sum
        return sum
    }
}

func main() {
    f := fibonacci()
    for i := 0; i &lt; 10; i   {
        fmt.Println(f())
    }
}
</code></pre></li>
<li><p><a href="http://tour.golang.org/#48">Advanced Exercise: Complex cube roots</a></p>

<pre><code>package main

import (
    "fmt"
    "math"
    "cmath"
)

func Cbrt(x complex128) complex128 {
    var z complex128 = x
    for i:=0; i 1e-15 {
        z0 := z
        z = z-(z*z-f)/(2*z)
        d = z-z0
        if d &lt; 0 {
            d = -d
        }
    }
    return z, nil
}

func main() {
    if value, err := Sqrt(-2.0); err != nil {
        fmt.Println(err)
    } else {
        fmt.Println(value)
    }
}
</code></pre></li>
<li><p><a href="http://tour.golang.org/#59">Exercise: Images</a></p>

<pre><code>package main

import (
    "image"
    "tour/pic"
)

type Image struct {}

func (m *Image) ColorModel() image.ColorModel {
    return image.RGBAColorModel
}

func (m *Image) Bounds() image.Rectangle {
    return image.Rect(0,0,256,256)
}

func (m *Image) At(x, y int) image.Color {
    v:=uint8(x^y)
    return image.RGBAColor{v, v, 255, 255}
}

func main() {
    m := &amp;Image{}
    pic.ShowImage(m)
}
</code></pre></li>
<li><p><a href="http://tour.golang.org/#60">Exercise: Rot13 Reader</a></p>

<pre><code>package main

import (
    "io"
    "os"
    "strings"
)

type rot13Reader struct {
    r io.Reader
}

func rot13(b byte) byte { 
    switch { 
        case 'A' &lt;= b &amp;&amp; b &lt;= 'M': 
                b = (b - 'A')+'N' 
        case 'N' &lt;= b &amp;&amp; b &lt;= 'Z': 
                b = (b - 'N')+'A' 
        case 'a' &lt;= b &amp;&amp; b &lt;= 'm': 
                b = (b - 'a')+'n' 
        case 'n' &lt;= b &amp;&amp; b &lt;= 'z': 
                b = (b - 'n')+'a' 
    } 
    return b 
}

func (b *rot13Reader) Read(p []byte) (n int, err os.Error) {
    n, err = b.r.Read(p)
    for i:=range(p[:n]) {
        p[i]=rot13(p[i])
    }
    return
}

func main() {
    s := strings.NewReader(
        "Lbh penpxrq gur pbqr!")
    r := rot13Reader{s}
    io.Copy(os.Stdout, &amp;r)
}
</code></pre></li>
<li><p><a href="http://tour.golang.org/#69">Exercise: Equivalent Binary Trees</a></p>

<pre><code>package main

import (
    "fmt"
    "tour/tree"
)

// Walk walks the tree t sending all values
// from the tree to the channel ch.
func Walk(t *tree.Tree, ch chan int) {
    if t.Left != nil {
        Walk(t.Left, ch)
    }
    ch&lt;-t.Value
    if t.Right != nil {
        Walk(t.Right, ch)
    }
}

// Same determines whether the trees
// t1 and t2 contain the same values.
func Same(t1, t2 *tree.Tree) bool {
    ch1:=make(chan int)
    ch2:=make(chan int)
    go Walk(t1, ch1)
    go Walk(t2, ch2)
    for i:=0; i&lt;10; i++ {
        if &lt;-ch1 != &lt;-ch2 {
            return false
        }
    }
    return true
}   

func main() {
    ch := make(chan int)
    go Walk(tree.New(1), ch)

    for i:=0; i&lt;10; i++ {
        fmt.Println(&lt;-ch)
    }

    fmt.Println("Equivalent Binary Trees?",
        Same(tree.New(1), tree.New(1)))

    fmt.Println("Equivalent Binary Trees?",
        Same(tree.New(1), tree.New(2)))
}
</code></pre></li>
<li><p><a href="http://tour.golang.org/#70">Exercise: Web Crawler</a></p>

<pre><code>package main

import (
    "os"
    "fmt"
)

type Fetcher interface {
    // Fetch returns the body of URL and
    // a slice of URLs found on that page.
    Fetch(url string) (body string, urls []string, err os.Error)
}

// Crawl uses fetcher to recursively crawl
// pages starting with url, to a maximum of depth.
func Crawl(url string, depth int, fetcher Fetcher) {
    type target struct{
        urls  []string
        depth int
    }
    more := make(chan target)

    fetchPage := func(url string, depth int) {
        body, urls, err := fetcher.Fetch(url)
        if err != nil {
            fmt.Println(err)
        } else {
            fmt.Printf("found: %s %q\n", url, body)
        }
        more &lt;- target{urls, depth+1}
    }

    go fetchPage(url, 0)

    visited := map[string]bool{url:true}
    for counter := 1; counter &gt; 0; {
        next := &lt;-more
        counter--
        if next.depth &gt; depth {
            continue
        }
        for _, url := range next.urls {
            if _, seen := visited[url]; seen {
                continue
            }
            visited[url] = true
            counter++
            go fetchPage(url, next.depth)
        }
    }
}

func main() {
    Crawl("http://golang.org/", 4, fetcher)
}


// fakeFetcher is Fetcher that returns canned results.
type fakeFetcher map[string]*fakeResult

type fakeResult struct {
    body string
    urls     []string
}

func (f *fakeFetcher) Fetch(url string) (string, []string, os.Error) {
    if res, ok := (*f)[url]; ok {
        return res.body, res.urls, nil
    }
    return "", nil, fmt.Errorf("not found: %s", url)
}

// fetcher is a populated fakeFetcher.
var fetcher = &amp;fakeFetcher{
    "http://golang.org/": &amp;fakeResult{
        "The Go Programming Language",
        []string{
            "http://golang.org/pkg/",
            "http://golang.org/cmd/",
        },
    },
    "http://golang.org/pkg/": &amp;fakeResult{
        "Packages",
        []string{
            "http://golang.org/",
            "http://golang.org/cmd/",
            "http://golang.org/pkg/fmt/",
            "http://golang.org/pkg/os/",
        },
    },
    "http://golang.org/pkg/fmt/": &amp;fakeResult{
        "Package fmt",
        []string{
            "http://golang.org/",
            "http://golang.org/pkg/",
        },
    },
    "http://golang.org/pkg/os/": &amp;fakeResult{
        "Package os",
        []string{
            "http://golang.org/",
            "http://golang.org/pkg/",
        },
    },
}
</code></pre></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[363]]></title>
    <link href="http://sw0x2A.github.com/2011/09/363/"/>
    <updated>2011-09-15T00:00:00+02:00</updated>
    <id>http://sw0x2A.github.com/2011/09/363</id>
    <content type="html"><![CDATA[<h1></h1>

<p>I cannot open some SSL-encrypted websites (https) in Firefox right now. I have found out that this is caused by the Online Certificate Status Protocol (OCSP) which confirms the current validity of certificates. All those websites are using certificates issued by <a href="http://cacert.org">cacert.org</a> and their OCSP server (ocsp.cacert.org) is currently quiet busy (“Too many users” or timeouts).</p>

<p>Quick workaround is to temporarily disable OCSP in Firefox (or other affected web browsers). You can uncheck this feature in Preferences → Advanced → Encryption → Validation in Firefox.</p>

<p><strong>Update:</strong> Server seems to be working again.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Significant improvements to indexes in MongoDB v2.0]]></title>
    <link href="http://sw0x2A.github.com/2011/09/significant-improvements-to-indexes-in-mongodb-v2-0/"/>
    <updated>2011-09-13T00:00:00+02:00</updated>
    <id>http://sw0x2A.github.com/2011/09/significant-improvements-to-indexes-in-mongodb-v2-0</id>
    <content type="html"><![CDATA[<h1></h1>

<p>I have upgraded my MongoDB installation to version 2.0 today and converted existing indexes to {v:1} format. According to the <a href="http://www.mongodb.org/display/DOCS/2.0">release notes</a>, <em>“Indexes are often 25% smaller and 25% faster (depends on the use case)”</em>.</p>

<pre><code>&gt; db.hashes.totalIndexSize()
5932986112
&gt; db.hashes.runCommand( "compact" )
{ "ok" : 1 }
&gt; db.hashes.totalIndexSize()
3527294016
</code></pre>

<p>In my case, a more than 10 GB database, the <code>totalIndexSize</code> of {v:1} format indexes is reduced by more than 40% to only 59.45% of the size of the old {v:0} format indexes. Well done, MongoDB developers!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Instant query log for MySQL]]></title>
    <link href="http://sw0x2A.github.com/2011/09/instant-query-log-for-mysql/"/>
    <updated>2011-09-06T00:00:00+02:00</updated>
    <id>http://sw0x2A.github.com/2011/09/instant-query-log-for-mysql</id>
    <content type="html"><![CDATA[<h1></h1>

<p>MySQL has <a href="http://dev.mysql.com/doc/refman/5.5/en/server-logs.html">build-in logging</a> for queries (general) and slow queries (slow). You can enable them at startup. It is not recommended to enable general logs on a high-traffic database server for performance reasons but sometimes you may want to be able to see what queries are executed there.</p>

<p>Since <a href="http://www.maatkit.org/doc/mk-query-digest.html">mk-query-digest</a> is able to read <a href="http://www.tcpdump.org/">tcpdumps</a>, it is the perfect tool to create an instant query log. Just pipe all network traffic on MySQL port to mk-query-digest for analysis.</p>

<pre><code>tcpdump -i eth2 port 3306 -s 65535  -x -n -q -tttt | 
  mk-query-digest --type tcpdump
</code></pre>

<p>There are much more things you can do with mk-query-digest, so it is worth to write a dump of network traffic to a file and work with it.</p>

<pre><code>tcpdump -i eth2 port 3306 -s 65535  -x -n -q -tttt &gt; tcpdump.out
</code></pre>

<p>You can convert it to slow query log format and parse it with your favorite analysis tool.</p>

<pre><code>mk-query-digest --type tcpdump --print --noreport &lt; tcpdump.out &gt; slow.log
</code></pre>

<p>The report of an analysis of mk-query-digest fingerprints queries. To search for them in your logfile, you can use the <code>--filter</code> parameter.</p>

<pre><code>mk-query-digest slow.log --no-report --print 
--filter '$event-&gt;{fingerprint} &amp;&amp; make_checksum($event-&gt;{fingerprint}) eq "76A68B0365255C58"'
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Logrotate with MongoDB]]></title>
    <link href="http://sw0x2A.github.com/2011/08/logrotate-with-mongodb/"/>
    <updated>2011-08-22T00:00:00+02:00</updated>
    <id>http://sw0x2A.github.com/2011/08/logrotate-with-mongodb</id>
    <content type="html"><![CDATA[<h1></h1>

<p>MongoDB packages are shipped with <a href="http://www.mongodb.org/display/DOCS/Logging">logging</a> enabled in configuration but without a script to rotate the logfile. There are two build-in ways to let MongoDB rotate its logfile. You can execute <code>db.runCommand("logRotate");</code> from the mongo shell or <code>kill -SIGUSR1 $(cat /var/lib/mongodb/mongod.lock)</code> from unix shell.</p>

<p>The best way to automate this is to use <code>logrotate</code>. Copy the following code into <code>/etc/logrotate.d/mongodb</code> and make sure that pathes and filenames in the script correspond with those in your system.</p>

<pre><code>/var/log/mongodb/mongodb.log {
    daily
    rotate 7
    compress
    missingok
    notifempty
    sharedscripts
    postrotate
        /bin/kill -SIGUSR1 `cat /var/lib/mongodb/mongod.lock` &amp;&amp; 
        rm -f /var/log/mongodb/mongodb.log.????-??-??T??-??-??
    endscript
}
</code></pre>

<p>It rotates the logfile <code>/var/log/mongodb/mongodb.log</code> on a daily basis and keeps them for 7 days compressed. Since MongoDB’s build-in <code>logRotate</code> conflicts with <code>logrotate</code>, it also cleans up the empty, log-rotated files created by MongoDB.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[340]]></title>
    <link href="http://sw0x2A.github.com/2011/08/340/"/>
    <updated>2011-08-22T00:00:00+02:00</updated>
    <id>http://sw0x2A.github.com/2011/08/340</id>
    <content type="html"><![CDATA[<h1></h1>

<p><a href="http://www.davidpashley.com/articles/writing-robust-shell-scripts.html">Writing robust Bash shell scripts</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[338]]></title>
    <link href="http://sw0x2A.github.com/2011/08/338/"/>
    <updated>2011-08-02T00:00:00+02:00</updated>
    <id>http://sw0x2A.github.com/2011/08/338</id>
    <content type="html"><![CDATA[<h1></h1>

<p><a href="http://www252.pair.com/comdog/mastering_perl/">Mastering Perl</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Solarize your terminal!]]></title>
    <link href="http://sw0x2A.github.com/2011/08/solarize-your-terminal/"/>
    <updated>2011-08-01T00:00:00+02:00</updated>
    <id>http://sw0x2A.github.com/2011/08/solarize-your-terminal</id>
    <content type="html"><![CDATA[<h1></h1>

<p>I have <a href="http://www.xaprb.com/blog/2011/07/28/easy-on-the-eyes-the-solarized-color-theme/">read</a> about <a href="http://ethanschoonover.com/solarized">Solarized</a> recently. It is a sixteen color palette designed for use with terminal and gui applications. I am using both the dark and light mode in <a href="https://github.com/sigurdga/gnome-terminal-colors-solarized">Gnome Terminal</a>, <a href="https://github.com/altercation/vim-colors-solarized">vim</a> and <a href="https://github.com/fentie/netbeans-colors-solarized">Netbeans</a> since a few days and I really like to work with it and do not want to miss it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Perl: Inserting a lot of documents in MongoDB]]></title>
    <link href="http://sw0x2A.github.com/2011/08/perl-inserting-a-lot-of-documents-in-mongodb/"/>
    <updated>2011-08-01T00:00:00+02:00</updated>
    <id>http://sw0x2A.github.com/2011/08/perl-inserting-a-lot-of-documents-in-mongodb</id>
    <content type="html"><![CDATA[<h1></h1>

<p>In the last few day, I spend a lot of time to figure out and learn how <a href="http://www.mongodb.org/">MongoDB</a> works and what is the best solution to fill a collection with millions of records (documents). To be more precise, I want to store unique strings and some other data related to that strings. One should be able to search for values in all fields, therefore I set indexes for all of them and a unique one for the strings.</p>

<p>Since I want to insert a lot of records, I thought it would be a good idea to use <code>batch_insert</code> (see <a href="http://api.mongodb.org/perl/current/MongoDB/Collection.html">API Documentation for Perl driver</a>), which would insert each document in an array into the database. It turned out very fast that I have to check for duplicate strings because <code>batch-insert</code> breaks immediately when it gets an error from the unique index and does not insert the remaining documents. I ended up with something like the following, which does what it should but I had a bad feeling and started to profile that code.</p>

<pre><code>while() {
    chomp(my $string = $_);
    next if defined($mongo_coll-&gt;find_one({ plaintext =&gt; $string }));
    push(@documents, { 
        "plaintext" =&gt;  "$string",
        "foo"       =&gt;  "bar",
    });
    if ( $counter   == $max_inserts ) {
        mdb_batch_insert(@documents);
        @hashes=(); $counter=1;
    }
}
mdb_batch_insert(@documents);

sub mdb_batch_insert {
    my $documents_ref = shift;
    $mongo_coll-&gt;batch_insert($documents_ref);
    my $err = $mongo_db-&gt;last_error();
    unless ( $err-&gt;{ok} eq 1 and not defined($err-&gt;{err}) ) {
        $logger-&gt;error(Dumper($err));
    }
}
</code></pre>

<p>For profiling (using <a href="http://search.cpan.org/perldoc?Devel::SmallProf">Devel::SmallProf</a>) and benchmarking I inserted the same 100000 documents twice into an empty collection. Doing so, I have got results for inserts without collisions with unique index and no inserts or inserts with 100% collisions.</p>

<p>The results pointed out that a lot of time was wasted with checking for duplicates and errors. I started to play around with other ways to insert data and was surprised to see that the easiest approach is also the fastest: Since MongoDB takes automatically care of duplicates, it is possible to just insert each document separately without checking for errors.</p>

<pre><code>while () {
    chomp( my $plaintext = $_ );
    $mongo_coll-&gt;insert({
        "plaintext" =&gt;  "$string",
        "foo"       =&gt;  "bar",
    });
}
</code></pre>

<p>I use that code to insert the first 23 million documents. After approximately 10 million inserts, I could see that performance of MongoDB and my script went down. Another problem was that some documents (~5000) were not inserted due to errors I did not checked for. What have I learned? That I have to check for errors and even when it is not a measurable problem in an almost empty database, of course, <code>insert</code> is more expensive than <code>find_one</code>.</p>

<pre><code>while () {
    chomp( my $plaintext = $_ );
    next if defined( $mongo_coll-&gt;find_one( { plaintext =&gt; $plaintext } ) );
    eval {
        $mongo_coll-&gt;insert(
            {
                        "plaintext" =&gt;  "$plaintext",
                        "foo"       =&gt;  "bar",
            },
            { safe =&gt; 1 }
        );
    };
    $logger-&gt;error($@) if $@;
}
</code></pre>

<p>At the end, one could say that it would have been obvious that the final solution is the best. Yes, it is how I would have done it with MySQL. But I wanted to become familiar with it and was wondering whether MongoDB would act similar or not. Now I can be sure.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[328]]></title>
    <link href="http://sw0x2A.github.com/2011/07/328/"/>
    <updated>2011-07-29T00:00:00+02:00</updated>
    <id>http://sw0x2A.github.com/2011/07/328</id>
    <content type="html"><![CDATA[<h1></h1>

<p>I have just <a href="http://code.google.com/p/mysql-cacti-templates/source/detail?r=565">commited another graph template</a> to mysql-cacti-templates.</p>
]]></content>
  </entry>
  
</feed>
